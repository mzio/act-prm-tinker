"""
Define EpisodeStep (transition), Trajectory (sequence of EpisodeStep),
TrajectoryGroup (batch of Trajectory)
"""

from typing import Any
from pydantic import BaseModel

from .utils import compute_returns_with_last_value


class EpisodeStep(BaseModel):
    """
    Single episode step, i.e., a transition between two states
    -> Note that tools is optional, but should be saved if available
    """
    state: list[dict[str, Any]]
    action: dict[str, Any]
    next_obs: list[dict[str, Any]]
    
    state_len: int
    state_action_tokens: list[int]
    old_logprobs: list[float]
    temperature: float

    reward: float
    done: bool
    truncated: bool
    timestep: int
    try_step: int

    batch_id: int
    unique_data_sample_id: int
    generation_id: int
    split: str

    return_: float = -2
    advantage: float = -2
    return_is_computed: bool = False
    advantage_is_computed: bool = False
    constant_reward_group: bool = False

    tools: list[dict[str, Any]] | None = None


class Trajectory():
    """
    Sequence of EpisodeSteps, generated by a complete rollout
    """
    def __init__(
        self,
        episode_steps: list[EpisodeStep],
        try_step: int,
        discount_factor: float,
        final_state: list[dict[str, Any]],
        final_obs: list[dict[str, Any]],
        final_reward: float,
        last_value: float = 0.0,
    ) -> None:
        self.episode_steps = episode_steps
        self.try_step = try_step
        self.discount_factor = discount_factor
        self.final_state = final_state
        self.final_obs = final_obs
        self.final_reward = final_reward
        self.last_value = last_value

        self.returns = self.compute_returns()
        self.first_return = self.returns[0]
        self.correct = self.final_reward == 1  # heuristic for accuracy evals

    def __len__(self) -> int:
        return len(self.episode_steps)

    def compute_returns(self) -> list[float]:
        """
        Compute returns for each episode step
        """
        if self.episode_steps[-1].return_is_computed:
            return [step.return_ for step in self.episode_steps]
        # Otherwise, compute the (discounted) returns
        returns: list[float] = compute_returns_with_last_value(
            rewards=[step.reward for step in self.episode_steps],
            discount_factor=self.discount_factor,
            dones=[step.done for step in self.episode_steps],
            last_value=self.last_value,
        )
        for _idx, return_ in enumerate(returns):
            self.episode_steps[_idx].return_ = return_
            self.episode_steps[_idx].return_is_computed = True
        return returns


class TrajectoryGroup():
    """
    Batch of Trajectories, generated by a group of rollouts
    """
    def __init__(
        self,
        trajectories: list[Trajectory],
        discount_factor: float,
        final_rewards: list[float] | None = None,
    ) -> None:
        self.trajectories = trajectories
        self.discount_factor = discount_factor
        self.final_rewards = (
            final_rewards or [trajectory.final_reward for trajectory in self.trajectories]
        )
        self.first_returns = self.get_first_returns()
        self.advantages = self.compute_advantages()

    def get_first_returns(self) -> list[float]:
        """
        Get the first returns for each trajectory, i.e.,
        the discounted sum of rewards at each step of the trajectory
        """
        # return [
        #     trajectory.compute_returns()[0] for trajectory in self.trajectories
        # ]
        return [trajectory.first_return for trajectory in self.trajectories]

    def compute_advantages(self) -> list[list[float]]:
        """
        Compute advantages for each episode step in the trajectory
        - By default, just use the returns as advantages
        """
        advantages = [
            trajectory.compute_returns() for trajectory in self.trajectories
        ]
        for t_idx, trajectory in enumerate(self.trajectories):
            for e_idx, _ in enumerate(trajectory.episode_steps):
                self.trajectories[t_idx].episode_steps[e_idx].advantage = advantages[t_idx][e_idx]
                self.trajectories[t_idx].episode_steps[e_idx].advantage_is_computed = True
        return advantages


class MeanCenteredTrajectoryGroup(TrajectoryGroup):
    """
    Batch of Trajectories, where we compute advantages by mean-centering the final rewards, i.e.,
    advantages = final_rewards - mean(final_rewards)

    If `discount_factor` < 1, we we compute step-wise advantages by applying step-wise discounting 
    *after* mean-centering the final returns (as an empirical hack to not having all MC rollouts 
    per state at each timestep)
    """

    def compute_advantages(self) -> list[list[float]]:
        """
        Compute advantages for each episode step in the trajectory as 
        (discounted) mean-centered final returns
        """
        # final_rewards_in_group: list[float] = [
        #     trajectory.final_reward for trajectory in self.trajectories_G
        # ]
        mean_reward: float = sum(self.final_rewards) / len(self.final_rewards)
        if self.discount_factor == 1:  # no discounting, advantage per rollout same for all steps
            return [
                [(trajectory.final_reward - mean_reward) for _ in range(len(trajectory))]
                for trajectory in self.trajectories
            ]
        # Otherwise, apply step-wise discounting after mean-centering
        advantages: list[list[float]] = [
            [
                (traj.final_reward - mean_reward) * (self.discount_factor ** (len(traj) - t - 1))
                for t in range(len(traj))
            ] for traj in self.trajectories
        ]
        for t_idx, trajectory in enumerate(self.trajectories):
            for e_idx, _ in enumerate(trajectory.episode_steps):
                self.trajectories[t_idx].episode_steps[e_idx].advantage = advantages[t_idx][e_idx]
                self.trajectories[t_idx].episode_steps[e_idx].advantage_is_computed = True
        return advantages
